#!/usr/bin/env bash

set -eo pipefail
# set -x

DC="${DC:-exec}"

# If we're running in CI we need to disable TTY allocation for docker-compose
# commands that enable it by default, such as exec and run.
TTY=""
if [[ ! -t 1 ]]; then
  TTY="-T"
fi

# -----------------------------------------------------------------------------
# Helper functions start with _ and aren't listed in this script's help menu.
# -----------------------------------------------------------------------------

function _dc {
  docker-compose "${DC}" ${TTY} "${@}"
}

# function docker-compose {
#   # docker-compose --env-file '.env' "${@}"
#   docker-compose "${@}"
# }

# -----------------------------------------------------------------------------

function cmd {
  # Run any command you want in the worker container
  # _dc worker "${@}"
  _dc --rm --no-deps worker "${@}"
}

# function worker-cmd {
#   # Run any command you want in the worker container
#   # _dc worker "${@}"
#   _dc --rm --no-deps worker "${@}"
# }

function bash {
  # Start a Bash session in the worker container
  cmd bash "${@}"
}

function sharectl {
  cmd sharectl "${@}"
}

# -----------------------------------------------------------------------------

DATASET_PUBMED_DIR=/media/SITCH_DATA_14TB/datasets/ftp.ncbi.nlm.nih.gov/pubmed

SYM_DIR=/media/SITCH_M2_1TB/datasets/share.osf.io
# SYM_DIR=/media/SITCH_DATA_14TB/datasets/share.osf.io

SYM_DOTENV="${SYM_DIR}/.env"
SYM_FETCHED_DIR="${SYM_DIR}/fetched"

# FETCHED=/home/sitch/sites/fortress/SHARE/fetched
# FETCHED_GOV_NIH_DIR=/home/sitch/sites/fortress/SHARE/fetched/gov.nih
# FETCHED_GOV_PUBMEDCENTRAL_DIR=/home/sitch/sites/fortress/SHARE/fetched/gov.pubmedcentral
# FETCHED_GOV_CLINICALTRIALS_DIR=/home/sitch/sites/fortress/SHARE/fetched/gov.clinicaltrials

DOTENV="$(pwd)/.env"

FETCHED_DIR="$(pwd)/fetched"
FETCHED_GOV_NIH_DIR="${FETCHED_DIR}/gov.nih"
FETCHED_GOV_PUBMEDCENTRAL_DIR="${FETCHED_DIR}/gov.pubmedcentral"
FETCHED_GOV_CLINICALTRIALS_DIR="${FETCHED_DIR}/gov.clinicaltrials"

POSTGRES_DATA_VOL=/media/SITCH_M2_1TB/db/share.osf.io/postgresql/data

function test-symlink {
  if [ -L ${@} ]; then
    if [ -e ${@} ]; then
      echo "[symlink] Good link"
    else
      echo "[symlink] Broken link"
    fi
  elif [ -e ${@} ]; then
    echo "[symlink] Not a symlink"
  else
    echo "[symlink] Missing"
  fi
}

function init-datasets {
  link-dotenv
  link-datasets
  fetch-dataset-pubmed
}

function link-dotenv {
  if [ ! -L ${DOTENV} ] || [ ! -e ${DOTENV} ]; then
    echo "Creating symlink ${SYM_DOTENV} -> ${DOTENV}"
    ln -s "${SYM_DOTENV}" "${DOTENV}"
  else
    echo "Symlink exists, skipping ${SYM_DOTENV} ${DOTENV}"
    test-symlink "${DOTENV}"
  fi
}

function link-datasets {
  if [ ! -L ${FETCHED_DIR} ] || [ ! -e ${FETCHED_DIR} ]; then
    echo "Creating symlink ${SYM_FETCHED_DIR} -> ${FETCHED_DIR}"
    ln -s "${SYM_FETCHED_DIR}" "${FETCHED_DIR}"
  else
    echo "Symlink exists, skipping ${SYM_FETCHED_DIR} ${FETCHED_DIR}"
    test-symlink "${FETCHED_DIR}"
  fi

  mkdir -p "${FETCHED_GOV_NIH_DIR}"
  mkdir -p "${FETCHED_GOV_CLINICALTRIALS_DIR}"
  mkdir -p "${FETCHED_GOV_PUBMEDCENTRAL_DIR}"
}

function rsync-copy {
  rsync --archive --human-readable --partial --info=stats1,progress2 --modify-window=1 "${@}"
}

function fetch-dataset-pubmed {
  mkdir -p "${FETCHED_GOV_PUBMEDCENTRAL_DIR}"

  echo "Copying PubMED baseline..."
  rsync-copy --include='*.xml.gz' --exclude='*' "${DATASET_PUBMED_DIR}/baseline/" "${FETCHED_GOV_PUBMEDCENTRAL_DIR}/"

  echo "Copying PubMED updatefiles..."
  rsync-copy --include='*.xml.gz' --exclude='*' "${DATASET_PUBMED_DIR}/updatefiles/" "${FETCHED_GOV_PUBMEDCENTRAL_DIR}/"

  # Keep gz files
  find "${FETCHED_GOV_PUBMEDCENTRAL_DIR}/" -type f -name '*.xml.gz' -print -exec unpigz --keep {} \;

  # find "${FETCHED_GOV_PUBMEDCENTRAL_DIR}/" -type f -name '*.xml.gz' -print -exec unpigz {} \;
}

# -----------------------------------------------------------------------------

function init-network {
  docker network create share_default --subnet 172.24.24.0/24
}

function init-elasticsearch {
  docker-compose run --rm --no-deps worker sharectl search setup --initial
}

function migrate {
  docker-compose up -d postgres elasticsearch
  docker-compose run --rm --no-deps worker python manage.py migrate
}

# -----------------------------------------------------------------------------

function init {
  # init-datasets
  init-network
  setup

  # ln -s /media/SITCH_DATA_14TB/datasets/share.osf.io/fetched "${pwd}/fetched"
}

function setup {
  docker-compose pull
  docker-compose up requirements

  # docker-compose up -d postgres elasticsearch

  migrate

  # docker-compose up -d rabbitmq
  # docker-compose up -d worker

  init-elasticsearch

  # docker-compose up -d --build web

  # docker-compose up -d indexer
  # docker-compose up -d frontend

  down
}

function db-reset {
  docker-compose down
  rm-postgres-data-vol

  migrate
  docker-compose down
}

function rm-postgres-data-vol {
  echo "Resetting ${POSTGRES_DATA_VOL}"
  sudo rm -rf "${POSTGRES_DATA_VOL}"
  mkdir -p "${POSTGRES_DATA_VOL}"
  sudo chown postgres:postgres "${POSTGRES_DATA_VOL}"
}

# -----------------------------------------------------------------------------

function _build_run_down {
  docker-compose build
  docker-compose run ${TTY} "${@}"
  docker-compose down
}

function up {
  docker-compose pull
  # docker-compose up requirements
  docker-compose up -d postgres elasticsearch

  # Migrate DBs
  migrate

  docker-compose up -d rabbitmq
  docker-compose up -d worker
  # docker-compose up -d web
  docker-compose up -d --build web
  docker-compose up -d indexer
  docker-compose up -d frontend
}

function down {
  docker-compose down
}

function console {
  docker-compose run --rm --no-deps worker bash
}

function django {
  docker-compose run --rm --no-deps worker python manage.py shell_plus
}

function logs {
  docker-compose logs -f worker
}

# function psql {
#   # Connect to PostgreSQL with psql
#   # shellcheck disable=SC1091
#   . .env
#   _dc postgres psql -U "${POSTGRES_USER}" "${@}"
# }

# function test {
#   # Run test suite
#   _dc -e "MIX_ENV=test" web mix test "${@}"
# }

# Source.objects.filter(name='gov.nih').update(canonical=True, full_harvest=True, disabled=False)
# Source.objects.filter(name='gov.clinicaltrials').update(canonical=True, full_harvest=True, disabled=False)
# Source.objects.filter(name='gov.pubmedcentral').update(canonical=True, full_harvest=True, disabled=False)

# sharectl schedule 'gov.clinicaltrials' --start=2021-09-20 --end=2021-09-21 --run
# sharectl schedule 'gov.clinicaltrials' 2021-11-01 --tasks

function enqueue-harvesters {
  sharectl schedule 'gov.nih' --start=1985-01-01 --end=2021-11-14 --tasks
  sharectl schedule 'gov.pubmedcentral.pmc' --start=1999-01-01 --end=2021-11-14 --tasks

  # Not working yet
  # sharectl schedule 'gov.clinicaltrials' --start=1999-09-20 --end=2021-11-14 --tasks

  # Not yet setup
  #   # sharectl schedule 'com.nature' --start=1990-01-01 --end=2021-11-14
}

# -----------------------------------------------------------------------------

function git-merge-upstream-repo-into-fork {
  git checkout master
  git pull https://github.com/CenterForOpenScience/SHARE.git master
  git push origin master

  git checkout develop
  git pull https://github.com/CenterForOpenScience/SHARE.git develop
  git push origin develop
}

# -----------------------------------------------------------------------------

function help {
  printf "%s <task> [args]\n\nTasks:\n" "${0}"

  compgen -A function | grep -v "^_" | cat -n

  printf "\nExtended help:\n  Each task has comments for general usage\n"
}

# This idea is heavily inspired by: https://github.com/adriancooney/Taskfile
TIMEFORMAT=$'\nTask completed in %3lR'
time "${@:-help}"
